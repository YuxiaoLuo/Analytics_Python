{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7916152f",
   "metadata": {},
   "source": [
    "# API vs Web Craping\n",
    "\n",
    "- Extracting informaiton from websites can be done via scraping or by working with the site API if there is one \n",
    "    - working with APIs is preferable \n",
    "    - Comparison of Web Scraping vs. API for Hacker News\n",
    "    \n",
    "## RAPTOR \n",
    "Raptor means: Review - Access - Parse - Transorm -Store.\n",
    "\n",
    "| |Web Server | Web Server + API|\n",
    "|:---|:---------|:-----------|\n",
    "|Review | HTML structure (tags, attributes, etc.) | Parameters and structure from documentation|\n",
    "\n",
    "\n",
    "### Hacker News Example\n",
    "[Hacker News](https://news.ycombinator.com/) is a social Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by the investment fund and startup incubator Y Combinator. In general, content that can be submitted is defined as \"anything that gratifies one's intellectual curiosity.\"\n",
    "\n",
    "- It also offers an API providing structured, JSON-formatted results\n",
    "    - Base URL: https://hacker-news.firebaseio.com/v0\n",
    " \n",
    "- See explanation and documentation at: http://github.com/HackerNews/API\n",
    "\n",
    "- The new Python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364ca53",
   "metadata": {},
   "source": [
    "1. First, let's try to scrape all the article title, link, and score from https://news.ycombinator.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "423bbec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Software Engineering at Google (abseil.io)', 'Link': 'https://abseil.io/resources/swe-book/html/toc.html', 'Score': '24 points'}\n",
      "{'Title': 'Analysis of the data job market using HN job posts (emiruz.com)', 'Link': 'https://emiruz.com/post/2023-08-12-data-jobs/', 'Score': '22 points'}\n",
      "{'Title': 'The 2002 Überlingen midair collision (admiralcloudberg.medium.com)', 'Link': 'https://admiralcloudberg.medium.com/tears-in-the-rain-the-2002-%C3%BCberlingen-midair-collision-591232d0c51e', 'Score': '45 points'}\n",
      "{'Title': 'How to run a miserable code review (badsoftwareadvice.substack.com)', 'Link': 'https://badsoftwareadvice.substack.com/p/how-to-run-a-miserable-code-review', 'Score': '10 points'}\n",
      "{'Title': 'Nobody ever paid me for code (bitecode.dev)', 'Link': 'https://www.bitecode.dev/p/nobody-ever-paid-me-for-code', 'Score': '108 points'}\n",
      "{'Title': 'Writing about what you learn pushes you to understand topics better (addyosmani.com)', 'Link': 'https://addyosmani.com/blog/write-learn/', 'Score': '349 points'}\n",
      "{'Title': 'Show HN: Little Rat – Chrome extension monitors network calls of all extensions (github.com/dnakov)', 'Link': None, 'Score': '14 points'}\n",
      "{'Title': 'Svix (YC W21) Is Hiring a Founding Account Executive (US Remote) (svix.com)', 'Link': None, 'Score': '0 points'}\n",
      "{'Title': 'Launch HN: Serra (YC S23) – Open-source, Python-based dbt alternative', 'Link': None, 'Score': '36 points'}\n",
      "{'Title': 'Bypassing YouTube video download throttling (0x7d0.dev)', 'Link': 'https://blog.0x7d0.dev/history/how-they-bypass-youtube-video-download-throttling/', 'Score': '269 points'}\n",
      "{'Title': 'Automatically classifying the content of sound files using ML (gingerbeardman.com)', 'Link': 'https://blog.gingerbeardman.com/2023/08/13/automatically-classifying-the-content-of-sound-files-using-ml/', 'Score': '9 points'}\n",
      "{'Title': 'Testing on production (marcochiappetta.medium.com)', 'Link': 'https://marcochiappetta.medium.com/yes-you-should-test-on-production-61f6dc61908b', 'Score': '112 points'}\n",
      "{'Title': 'A video game where you are an operating system (plbrault.com)', 'Link': 'https://plbrault.com/blog-posts/i-created-the-nerdierst-game-ever-en/', 'Score': '537 points'}\n",
      "{'Title': 'Python: Just Write SQL (joaodlf.com)', 'Link': 'https://joaodlf.com/python-just-write-sql', 'Score': '96 points'}\n",
      "{'Title': 'Police raid of a Kansas newsroom raises alarms about violations of press freedom (npr.org)', 'Link': 'https://www.npr.org/2023/08/14/1193676139/newspaper-marion-county-kansas-police-raid-first-amendment', 'Score': '114 points'}\n",
      "{'Title': 'Positive association between altitude and suicide in U.S. counties (nih.gov)', 'Link': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3114154/', 'Score': '33 points'}\n",
      "{'Title': \"I built a garbage collector for a language that doesn't need one (claytonwramsey.github.io)\", 'Link': 'https://claytonwramsey.github.io/2023/08/14/dumpster.html', 'Score': '8 points'}\n",
      "{'Title': 'Bomb threat causes mass evacuation at DEF CON hacking convention (theregister.com)', 'Link': 'https://www.theregister.com/2023/08/14/def_con_roundup/', 'Score': '26 points'}\n",
      "{'Title': 'Is Venus in some way tidally locked to Earth? (2020) (astronomy.stackexchange.com)', 'Link': 'https://astronomy.stackexchange.com/questions/36488/is-venus-in-some-way-tidally-locked-to-earth', 'Score': '79 points'}\n",
      "{'Title': 'Fediverse and Naive Bayes: The Grandfather of Spam Filters Still Making Waves (pixelfed.blog)', 'Link': 'https://pixelfed.blog/p/2023/feature/autospam-and-naive-bayes-the-grandfather-of-spam-filters-still-making-waves', 'Score': '93 points'}\n",
      "{'Title': 'Show HN: A website chatbot that also uses APIs (chatwith.tools)', 'Link': 'https://chatwith.tools', 'Score': '9 points'}\n",
      "{'Title': 'Things Unix can do atomically (2010) (rcrowley.org)', 'Link': 'https://rcrowley.org/2010/01/06/things-unix-can-do-atomically.html', 'Score': '119 points'}\n",
      "{'Title': 'Long live the King of Kings Accession ritual in ancient Persia (britishmuseum.org)', 'Link': 'https://www.britishmuseum.org/blog/long-live-king-kings-accession-ritual-ancient-persia', 'Score': '19 points'}\n",
      "{'Title': 'Veilid is an open-source, P2P, mobile-ﬁrst, networked application framework (veilid.com)', 'Link': 'https://veilid.com/', 'Score': '92 points'}\n",
      "{'Title': 'The world in which IPv6 was a good design (2017) (apenwarr.ca)', 'Link': 'https://apenwarr.ca/log/20170810', 'Score': '140 points'}\n",
      "{'Title': 'Risky Giant Steps Can Solve Optimization Problems Faster (quantamagazine.org)', 'Link': 'https://www.quantamagazine.org/risky-giant-steps-can-solve-optimization-problems-faster-20230811/', 'Score': '42 points'}\n",
      "{'Title': 'Momentum, a DIY 12-voice virtual analogue polyphonic synthesizer based on TSynth (electrotechnique.cc)', 'Link': 'http://electrotechnique.cc/Momentum.html', 'Score': '27 points'}\n",
      "{'Title': 'Semantic Code Reviews – Simple and direct comments without drama (m31coding.com)', 'Link': 'https://www.m31coding.com/blog/semantic-reviews.html', 'Score': '54 points'}\n",
      "{'Title': 'Azure ChatGPT: Private and secure ChatGPT for internal enterprise use (github.com/microsoft)', 'Link': None, 'Score': '808 points'}\n",
      "{'Title': 'Jitsi Meet Flutter SDK (jitsi.org)', 'Link': 'https://jitsi.org/blog/introducing-the-jitsi-meet-flutter-sdk/', 'Score': '161 points'}\n"
     ]
    }
   ],
   "source": [
    "# source: adpated from Broucke & Baessen (Chp. b9)\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# articles is an list that will hold info about each article \n",
    "articles = []\n",
    "\n",
    "url = 'http://news.ycombinator.com/news'\n",
    "r = requests.get(url)\n",
    "html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# get all rows in news table\n",
    "for item in html_soup.find_all('tr', attrs = {'class':'athing'}):\n",
    "    \n",
    "    # scrape the title of each news \n",
    "    item_title = item.find('td', attrs = {'class':'title'}).find_next_sibling('td', attrs = {'class':'title'}).text\n",
    "    # find the hyperlink tag of each news\n",
    "    item_a = item.find('a', attrs = {'rel':'noreferrer'})\n",
    "    # extract href attribute from hyperlink tag\n",
    "    item_link = item_a.get('href') if item_a else None\n",
    "    # find the span tag with scores\n",
    "    item_score = item.find('span', attrs = {'class':'score'})\n",
    "    # find the row next to the above row and get scores\n",
    "    next_row = item.find_next_sibling('tr')\n",
    "    item_score = next_row.find('span', attrs = {'class':'score'})\n",
    "    item_score = item_score.get_text(strip = True) if item_score else '0 points'\n",
    "    \n",
    "    articles.append({\"Title\":item_title, \"Link\": item_link, \"Score\": item_score})\n",
    "    \n",
    "# append the article info \n",
    "for article in articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78027da",
   "metadata": {},
   "source": [
    "2. Let's try to use the Hacker News API to scrape all the articles from the news website. We are using HTTP request to obtain the response from API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74158900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "articles = []\n",
    "url = 'https://hacker-news.firebaseio.com/v0'\n",
    "\n",
    "# let's add the top stories element based on the API official document\n",
    "top_stories = requests.get(url + '/topstories.json').json()\n",
    "\n",
    "# see how many IDs we get\n",
    "print(len(top_stories))\n",
    "type(top_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa532043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37121180, 37120874, 37120911, 37120372, 37120967]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are ids of the news\n",
    "top_stories[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53c5b7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://hacker-news.firebaseio.com/v0/item/37121180.json\n",
      "Fetching: https://hacker-news.firebaseio.com/v0/item/37120874.json\n",
      "Fetching: https://hacker-news.firebaseio.com/v0/item/37120911.json\n",
      "Fetching: https://hacker-news.firebaseio.com/v0/item/37120372.json\n",
      "Fetching: https://hacker-news.firebaseio.com/v0/item/37120967.json\n"
     ]
    }
   ],
   "source": [
    "for story_id in top_stories[:5]:\n",
    "    story_url = url + '/item/{}.json'.format(story_id)\n",
    "    print(\"Fetching:\", story_url)\n",
    "    \n",
    "    # make http request to each story URL\n",
    "    r = requests.get(story_url)\n",
    "    # your response is json-encoded\n",
    "    story_dict = r.json()\n",
    "    # store each story info in a list\n",
    "    articles.append(story_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "71e385b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineering at Google https://abseil.io/resources/swe-book/html/toc.html 39\n",
      "Analysis of the data job market using HN job posts https://emiruz.com/post/2023-08-12-data-jobs/ 25\n",
      "How to run a miserable code review https://badsoftwareadvice.substack.com/p/how-to-run-a-miserable-code-review 16\n",
      "The 2002 Überlingen midair collision https://admiralcloudberg.medium.com/tears-in-the-rain-the-2002-%C3%BCberlingen-midair-collision-591232d0c51e 48\n",
      "I built a garbage collector for a language that doesn't need one https://claytonwramsey.github.io/2023/08/14/dumpster.html 13\n",
      "Writing about what you learn pushes you to understand topics better https://addyosmani.com/blog/write-learn/ 354\n",
      "Nobody ever paid me for code https://www.bitecode.dev/p/nobody-ever-paid-me-for-code 112\n",
      "Svix (YC W21) Is Hiring a Founding Account Executive (US Remote) https://www.svix.com/careers/ 1\n",
      "Show HN: Little Rat – Chrome extension monitors network calls of all extensions https://github.com/dnakov/little-rat 17\n",
      "Inside The Decline of Stack Exchange https://www.thediff.co/archive/inside-the-decline-of-stack-exchange/ 6\n"
     ]
    }
   ],
   "source": [
    "# display the key information you want\n",
    "for article in articles[:10]:\n",
    "    print(article['title'], article['url'], article['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed862f32",
   "metadata": {},
   "source": [
    "### How to retrieve the top 10 stories \n",
    "First method: query encoding in API requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2b274af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37121180, 37120982, 37120874, 37120911, 37120967, 37120372, 37119942, 37118883, 37120715, 37108833]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url10 = 'https://hacker-news.firebaseio.com/v0/topstories.json?limitToFirst=10&orderBy=\"$key\"'\n",
    "ten_top_stories = requests.get(url10).json()\n",
    "print(ten_top_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f96bcac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37121180.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37120982.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37120874.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37120911.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37120967.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37120372.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37119942.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37118883.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37120715.json\n",
      "Fetching:https://hacker-news.firebaseio.com/v0/item/37108833.json\n"
     ]
    }
   ],
   "source": [
    "url = 'https://hacker-news.firebaseio.com/v0'\n",
    "articles = []\n",
    "\n",
    "for story_id in ten_top_stories:\n",
    "    \n",
    "    # create article link for each \n",
    "    story_url = url + '/item/{}.json'.format(story_id)\n",
    "    print('Fetching: ' + story_url)\n",
    "    \n",
    "    r = requests.get(story_url)\n",
    "    story_dict = r.json()\n",
    "    \n",
    "    articles.append(story_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d34e3623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineering at Google https://abseil.io/resources/swe-book/html/toc.html 82\n",
      "Inside The Decline of Stack Exchange https://www.thediff.co/archive/inside-the-decline-of-stack-exchange/ 32\n",
      "Analysis of the data job market using HN job posts https://emiruz.com/post/2023-08-12-data-jobs/ 39\n",
      "How to run a miserable code review https://badsoftwareadvice.substack.com/p/how-to-run-a-miserable-code-review 25\n",
      "I built a garbage collector for a language that doesn’t need one https://claytonwramsey.github.io/2023/08/14/dumpster.html 30\n",
      "The 2002 Überlingen midair collision https://admiralcloudberg.medium.com/tears-in-the-rain-the-2002-%C3%BCberlingen-midair-collision-591232d0c51e 59\n",
      "Show HN: Little Rat – Chrome extension monitors network calls of all extensions https://github.com/dnakov/little-rat 31\n",
      "Writing about what you learn pushes you to understand topics better https://addyosmani.com/blog/write-learn/ 366\n",
      "Svix (YC W21) Is Hiring a Founding Account Executive (US Remote) https://www.svix.com/careers/ 1\n",
      "Nobody ever paid me for code https://www.bitecode.dev/p/nobody-ever-paid-me-for-code 115\n"
     ]
    }
   ],
   "source": [
    "# display the key information you want\n",
    "for article in articles:\n",
    "    print(article['title'], article['url'], article['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07a90c",
   "metadata": {},
   "source": [
    "The other approach consists of defining a dict and pass it as a parameter\n",
    "   - this along with the headers allows to make a more specific request to an API\n",
    "   - it's recommended when developer key is needed \n",
    "   \n",
    "### Specific API requests\n",
    " \n",
    "- To make the API requests more specific, use headers and parameters in the request\n",
    "     - Headers\n",
    "     - Parameters are like filters to modify the scope of the request\n",
    "         - check API documentation\n",
    "         \n",
    "- Reddit API\n",
    "    - Scape the news in Reddit \n",
    "    - With a user-agent header \n",
    "    - Is the Reddit API free?\n",
    "        - Not all apps on Reddit will have to pay. The following conditions, effective as of June 1, enable free access to the data API: Apps that make fewer than 100 queries per minute using OAuth authentication and 10 queries per minute not using OAuth can use the API free of charge ([source](https://www.techtarget.com/whatis/feature/Reddit-pricing-API-charge-explained?Offer=abt_pubpro_AI-Insider)).\n",
    "     - Reddit API official document: https://www.reddit.com/dev/api/.\n",
    "     \n",
    "- Dealing with json:\n",
    "    - `pprint`: The pprint module in Python is a utility module that you can use to print data structures in a readable, pretty way. It's a part of the standard library that's especially useful for debugging code dealing with API requests, large JSON files, and data in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca5694",
   "metadata": {},
   "source": [
    "1. Let's scrape the news in Reddit with API. \n",
    "\n",
    "    - http://www.reddit.com/r/news: The place for news articles about current events in the United States and the rest of the world.\n",
    "    \n",
    "    - http://www.reddit.com/r/Baruch: Baruch College's student run subreddit.\n",
    "\n",
    "Note: this example is adapted from this tutorial on [towardsdatascience](http://towardsdatascience.com/a-beginners-guide-to-accessing-data-withweb-apis-using-python-23d262181467)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9f2f91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do I have to apply for TAP in order to apply for the Excelsior Program ?\n",
      "Does anyone know how long it takes Baruch to get CLEP scores?\n",
      "Thoughts on Jessica Webster for LIB 3030?\n",
      "Has anyone had Linda Dukette for Bus 9558? What's the course load for bus 9558 in general?\n",
      "just made my fall 2023 schedule! any feedback on these courses/professors?\n"
     ]
    }
   ],
   "source": [
    "# import the packages\n",
    "import requests, json\n",
    "\n",
    "payload = {\n",
    "    'limit': 5,\n",
    "    't': 'hot'}\n",
    "\n",
    "headers = {\n",
    "    'User-agent': 'Reddit bot 1.0'}\n",
    "\n",
    "endpoint = 'http://www.reddit.com/r/news/top.json'\n",
    "\n",
    "# can try other channel\n",
    "# endpoint = 'http://www.reddit.com/r/funny/top.json'\n",
    "endpoint = 'http://www.reddit.com/r/Baruch/new.json'\n",
    "\n",
    "r = requests.get(endpoint, headers = headers, params = payload)\n",
    "r_json = json.loads(r.content)\n",
    "\n",
    "# USE pprint to figure the hierarchy in json data\n",
    "# import pprint\n",
    "# pprint.pprint(r)\n",
    "\n",
    "# extract elements from json data\n",
    "for sub in (r_json['data']['children']):\n",
    "    title = sub['data']['title']\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a1cc8add",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "# pprint.pprint(r.json()['data']['children'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a472ec0",
   "metadata": {},
   "source": [
    "### Authenticatoin for News API\n",
    "\n",
    "News API is a simple HTTP REST (REpresentational State Transfer) API for searching and retrieving live articles from various sources\n",
    "\n",
    "REST means architectural constraints and here is an article aboout its components: https://restfulapi.net/.\n",
    "\n",
    "- Get your secret API key\n",
    "    - Go to: https://newsapi.org/docs/get-started\n",
    "\n",
    "- MY API key: [*****************************************](e66483a4f4b0480a9a29a4fac0586469)\n",
    "\n",
    "- Read the terms of service: https://newsapi.org/terms\n",
    "    - Don't violate the use term\n",
    "    - attribution: The attribution should preferrably be a hyperlink to https://newsapi.org with the text \"Powered by News API\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a19c23d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newsapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-30f2852ff3bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnewsapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNewsApiClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnewsapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNewsApiClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'e66483a4f4b0480a9a29a4fac0586469'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# /v2/top-headlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'newsapi'"
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key='e66483a4f4b0480a9a29a4fac0586469')\n",
    "\n",
    "# /v2/top-headlines\n",
    "top_headlines = newsapi.get_top_headlines(q='bitcoin',\n",
    "                                          sources='bbc-news,the-verge',\n",
    "                                          category='business',\n",
    "                                          language='en',\n",
    "                                          country='us')\n",
    "# /v2/everything\n",
    "all_articles = newsapi.get_everything(q='bitcoin',\n",
    "                                      sources='bbc-news,the-verge',\n",
    "                                      domains='bbc.co.uk,techcrunch.com',\n",
    "                                      from_param='2017-12-01',\n",
    "                                      to='2017-12-12',\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page=2)\n",
    "\n",
    "# /v2/top-headlines/sources\n",
    "\n",
    "sources = newsapi.get_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88cfc3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Covid: Record German cases as WHO warns of Europe deaths 56\n",
      "2 Climate change: Facebook fails to flag denial, study finds 58\n",
      "3 Ahmaud Arbery: Nearly all-white jury chosen in black jogger murder trial 72\n",
      "4 How a medieval English law affects the US gun control debate 60\n",
      "5 LA 'jetpack man' was probably a balloon 39\n",
      "6 'I hope people would be more sceptical today' - Knox 52\n",
      "7 First pill to treat Covid gets approval in UK 45\n",
      "8 High-risk Covid gene more common in South Asians 48\n",
      "9 Trump-Russia Steele dossier analyst charged with lying to FBI 61\n",
      "10 COP26: Indonesia criticises 'unfair' deal to end deforestation 62\n"
     ]
    }
   ],
   "source": [
    "# Ex.4\n",
    "# source: https://www.geeksforgeeks.org/fetching-top-news-using-news-api/\n",
    "# BBC news api with authorization header and parameters\n",
    "\n",
    "import requests \n",
    "\n",
    "# headers to store the API key\n",
    "headers = {'Authorization': 'e66483a4f4b0480a9a29a4fac0586469'}\n",
    "query_params = {\n",
    "      \"source\": \"bbc-news\",\n",
    "      \"sortBy\": \"top\"\n",
    "}\n",
    "main_url = \" https://newsapi.org/v1/articles\"\n",
    " \n",
    "# fetching data in json format\n",
    "res = requests.get(main_url, headers = headers, params=query_params)\n",
    "open_bbc_page = res.json()\n",
    " \n",
    "# getting all articles in a string article\n",
    "article = open_bbc_page[\"articles\"]\n",
    " \n",
    "# empty list to hold all trending news\n",
    "results = []\n",
    "     \n",
    "for ar in article:\n",
    "    results.append(ar[\"title\"])\n",
    "\n",
    "# printing all trending news       \n",
    "for i in range(len(results)):            \n",
    "    print(i + 1, results[i], len(results[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e3979cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package requests:\n",
      "\n",
      "NAME\n",
      "    requests\n",
      "\n",
      "DESCRIPTION\n",
      "    Requests HTTP Library\n",
      "    ~~~~~~~~~~~~~~~~~~~~~\n",
      "    \n",
      "    Requests is an HTTP library, written in Python, for human beings.\n",
      "    Basic GET usage:\n",
      "    \n",
      "       >>> import requests\n",
      "       >>> r = requests.get('https://www.python.org')\n",
      "       >>> r.status_code\n",
      "       200\n",
      "       >>> b'Python is a programming language' in r.content\n",
      "       True\n",
      "    \n",
      "    ... or POST:\n",
      "    \n",
      "       >>> payload = dict(key1='value1', key2='value2')\n",
      "       >>> r = requests.post('https://httpbin.org/post', data=payload)\n",
      "       >>> print(r.text)\n",
      "       {\n",
      "         ...\n",
      "         \"form\": {\n",
      "           \"key1\": \"value1\",\n",
      "           \"key2\": \"value2\"\n",
      "         },\n",
      "         ...\n",
      "       }\n",
      "    \n",
      "    The other HTTP methods are supported - see `requests.api`. Full documentation\n",
      "    is at <https://requests.readthedocs.io>.\n",
      "    \n",
      "    :copyright: (c) 2017 by Kenneth Reitz.\n",
      "    :license: Apache 2.0, see LICENSE for more details.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __version__\n",
      "    _internal_utils\n",
      "    adapters\n",
      "    api\n",
      "    auth\n",
      "    certs\n",
      "    compat\n",
      "    cookies\n",
      "    exceptions\n",
      "    help\n",
      "    hooks\n",
      "    models\n",
      "    packages\n",
      "    sessions\n",
      "    status_codes\n",
      "    structures\n",
      "    utils\n",
      "\n",
      "FUNCTIONS\n",
      "    check_compatibility(urllib3_version, chardet_version)\n",
      "\n",
      "DATA\n",
      "    __author_email__ = 'me@kennethreitz.org'\n",
      "    __build__ = 140545\n",
      "    __cake__ = '✨ 🍰 ✨'\n",
      "    __copyright__ = 'Copyright 2020 Kenneth Reitz'\n",
      "    __description__ = 'Python HTTP for Humans.'\n",
      "    __license__ = 'Apache 2.0'\n",
      "    __title__ = 'requests'\n",
      "    __url__ = 'https://requests.readthedocs.io'\n",
      "    codes = <lookup 'status_codes'>\n",
      "\n",
      "VERSION\n",
      "    2.25.1\n",
      "\n",
      "AUTHOR\n",
      "    Kenneth Reitz\n",
      "\n",
      "FILE\n",
      "    c:\\users\\yuxiao luo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(requests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
